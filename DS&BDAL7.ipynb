{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45f4da1-80c6-4b2e-b90f-74bb6b69d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a82811-55d0-4f94-9ebb-20e6d314887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f8a6a7-be03-44a7-b785-46fcd64d3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'sample.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23463521-8dd0-456d-b55f-5d2cea46eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': [text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029c9849-555a-4d16-b5c6-8aa0e6e9b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921b4359-8884-4d63-b572-b39f09e38b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eac19ab-e059-4a9d-a65f-6115045addaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['Text', 'analytics', 'is', 'the', 'process', 'of', 'analyzing', 'unstructured', 'text', 'data', 'to', 'derive', 'meaningful', 'insights', '.', 'It', 'involves', 'various', 'preprocessing', 'steps', 'such', 'as', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.', 'Once', 'the', 'text', 'is', 'preprocessed', ',', 'we', 'can', 'calculate', 'term', 'frequency', 'and', 'inverse', 'document', 'frequency', 'to', 'represent', 'the', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4169e1d-773c-4b5c-a99f-66ce60e8098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e3b753-cb08-415c-88f0-74296ce2c9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging: [('Text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('analyzing', 'VBG'), ('unstructured', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('to', 'TO'), ('derive', 'VB'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('various', 'JJ'), ('preprocessing', 'VBG'), ('steps', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.'), ('Once', 'IN'), ('the', 'DT'), ('text', 'NN'), ('is', 'VBZ'), ('preprocessed', 'VBN'), (',', ','), ('we', 'PRP'), ('can', 'MD'), ('calculate', 'VB'), ('term', 'NN'), ('frequency', 'NN'), ('and', 'CC'), ('inverse', 'JJ'), ('document', 'NN'), ('frequency', 'NN'), ('to', 'TO'), ('represent', 'VB'), ('the', 'DT'), ('document', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"POS Tagging:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b368f0-adbd-4081-8959-fb8187fb0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76615083-be22-4661-ae50-a581b703f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['Text', 'analytics', 'process', 'analyzing', 'unstructured', 'text', 'data', 'derive', 'meaningful', 'insights', '.', 'involves', 'various', 'preprocessing', 'steps', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.', 'text', 'preprocessed', ',', 'calculate', 'term', 'frequency', 'inverse', 'document', 'frequency', 'represent', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79f21451-b51d-4c24-bb21-838332e08a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d54541b-ecd5-429d-9335-6d275d67b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['text', 'analyt', 'process', 'analyz', 'unstructur', 'text', 'data', 'deriv', 'meaning', 'insight', '.', 'involv', 'variou', 'preprocess', 'step', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.', 'text', 'preprocess', ',', 'calcul', 'term', 'frequenc', 'invers', 'document', 'frequenc', 'repres', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd946acb-747e-4905-80cb-ad0454b4fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdad6743-bca8-4c14-b951-37c0960f0d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['Text', 'analytics', 'process', 'analyzing', 'unstructured', 'text', 'data', 'derive', 'meaningful', 'insight', '.', 'involves', 'various', 'preprocessing', 'step', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', '.', 'text', 'preprocessed', ',', 'calculate', 'term', 'frequency', 'inverse', 'document', 'frequency', 'represent', 'document', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d7a3b26-17ec-4b5f-a317-06af0428cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Term Frequency (TF)\n",
    "tf = FreqDist(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a225974f-189d-4596-a767-b1188a5f644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: <FreqDist with 29 samples and 40 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(\"TF:\", tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9102a579-ad14-4cec-a0a5-e2b2678d6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inverse Document Frequency (IDF)\n",
    "idf = {}\n",
    "total_documents = 1  # Assuming we have only one document\n",
    "for term in tf.keys():\n",
    "    doc_freq = sum(1 for doc in [stemmed_tokens] if term in doc)\n",
    "    idf[term] = math.log(total_documents / (doc_freq + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f55785d3-d1ac-4c54-8e53-ca58dce7dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: {'text': -0.6931471805599453, 'analyt': -0.6931471805599453, 'process': -0.6931471805599453, 'analyz': -0.6931471805599453, 'unstructur': -0.6931471805599453, 'data': -0.6931471805599453, 'deriv': -0.6931471805599453, 'meaning': -0.6931471805599453, 'insight': -0.6931471805599453, '.': -0.6931471805599453, 'involv': -0.6931471805599453, 'variou': -0.6931471805599453, 'preprocess': -0.6931471805599453, 'step': -0.6931471805599453, 'token': -0.6931471805599453, ',': -0.6931471805599453, 'po': -0.6931471805599453, 'tag': -0.6931471805599453, 'stop': -0.6931471805599453, 'word': -0.6931471805599453, 'remov': -0.6931471805599453, 'stem': -0.6931471805599453, 'lemmat': -0.6931471805599453, 'calcul': -0.6931471805599453, 'term': -0.6931471805599453, 'frequenc': -0.6931471805599453, 'invers': -0.6931471805599453, 'document': -0.6931471805599453, 'repres': -0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "print(\"IDF:\", idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b782b194-1f21-4d62-b60c-c73763a19bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF\n",
    "tfidf = {}\n",
    "for term, freq in tf.items():\n",
    "    tfidf[term] = freq * idf[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90730e20-0575-4ee5-9bac-7f28273fafaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: {'text': -2.0794415416798357, 'analyt': -0.6931471805599453, 'process': -0.6931471805599453, 'analyz': -0.6931471805599453, 'unstructur': -0.6931471805599453, 'data': -0.6931471805599453, 'deriv': -0.6931471805599453, 'meaning': -0.6931471805599453, 'insight': -0.6931471805599453, '.': -2.0794415416798357, 'involv': -0.6931471805599453, 'variou': -0.6931471805599453, 'preprocess': -1.3862943611198906, 'step': -0.6931471805599453, 'token': -0.6931471805599453, ',': -3.4657359027997265, 'po': -0.6931471805599453, 'tag': -0.6931471805599453, 'stop': -0.6931471805599453, 'word': -0.6931471805599453, 'remov': -0.6931471805599453, 'stem': -0.6931471805599453, 'lemmat': -0.6931471805599453, 'calcul': -0.6931471805599453, 'term': -0.6931471805599453, 'frequenc': -1.3862943611198906, 'invers': -0.6931471805599453, 'document': -1.3862943611198906, 'repres': -0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF:\", tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671f2e1-77cf-4d72-adc1-d7996302c8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
